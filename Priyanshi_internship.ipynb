{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bipolar Factory Intership Problem Statement Solution \n",
    "# Date : 26/04/2020\n",
    "# Time: 7:20pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "r = requests.get('https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html',timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if (gt IE 9)|!(IE)]> <!--><html lang=\"en\" class=\"no-js page-interactive section-opinion page-theme-standard tone-opinion page-interactive-default limit-small layout-xlarge app-interactive\" itemid=\"https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html\" itemtype=\"http://schema.org/NewsArticle\" itemscope xmlns:og=\"http://opengraphprotocol.org/schema/\"><!--<![endif]-->\n",
      "<!--[if IE 9]> <html lang=\"en\" class=\"no-js ie9 lt-ie10 page-interactive section-opinion page\n"
     ]
    }
   ],
   "source": [
    "print(r.text[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "soup = BeautifulSoup(r.text, 'html.parser')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all('span', attrs={'class':'short-desc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"short-desc\"><strong>Jan. 21 </strong>“I wasn't a fan of Iraq. I didn't want to go into Iraq.” <span class=\"short-truth\"><a href=\"https://www.buzzfeed.com/andrewkaczynski/in-2002-donald-trump-said-he-supported-invading-iraq-on-the\" target=\"_blank\">(He was for an invasion before he was against it.)</a></span></span>,\n",
       " <span class=\"short-desc\"><strong>Jan. 21 </strong>“A reporter for Time magazine — and I have been on their cover 14 or 15 times. I think we have the all-time record in the history of Time magazine.” <span class=\"short-truth\"><a href=\"http://nation.time.com/2013/11/06/10-things-you-didnt-know-about-time/\" target=\"_blank\">(Trump was on the cover 11 times and Nixon appeared 55 times.)</a></span></span>,\n",
       " <span class=\"short-desc\"><strong>Jan. 23 </strong>“Between 3 million and 5 million illegal votes caused me to lose the popular vote.” <span class=\"short-truth\"><a href=\"https://www.nytimes.com/2017/01/23/us/politics/donald-trump-congress-democrats.html\" target=\"_blank\">(There's no evidence of illegal voting.)</a></span></span>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting the news headlines with date and other information into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []  \n",
    "for result in results:  \n",
    "    date = result.find('strong').text[0:-1] + ', 2017'\n",
    "    lie = result.contents[1][1:-2]\n",
    "    explanation = result.find('a').text[1:-1]\n",
    "    url = result.find('a')['href']\n",
    "    records.append((date, lie, explanation, url))\n",
    "\n",
    "import pandas as pd  \n",
    "df = pd.DataFrame(records, columns=['date', 'lie', 'explanation', 'url'])  \n",
    "df['date'] = pd.to_datetime(df['date'])  \n",
    "df.to_csv('trump_lies.csv', index=False, encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lie</th>\n",
       "      <th>explanation</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-21</td>\n",
       "      <td>I wasn't a fan of Iraq. I didn't want to go in...</td>\n",
       "      <td>He was for an invasion before he was against it.</td>\n",
       "      <td>https://www.buzzfeed.com/andrewkaczynski/in-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-21</td>\n",
       "      <td>A reporter for Time magazine — and I have been...</td>\n",
       "      <td>Trump was on the cover 11 times and Nixon appe...</td>\n",
       "      <td>http://nation.time.com/2013/11/06/10-things-yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>Between 3 million and 5 million illegal votes ...</td>\n",
       "      <td>There's no evidence of illegal voting.</td>\n",
       "      <td>https://www.nytimes.com/2017/01/23/us/politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>Now, the audience was the biggest ever. But th...</td>\n",
       "      <td>Official aerial photos show Obama's 2009 inaug...</td>\n",
       "      <td>https://www.nytimes.com/2017/01/21/us/politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>Take a look at the Pew reports (which show vot...</td>\n",
       "      <td>The report never mentioned voter fraud.</td>\n",
       "      <td>https://www.nytimes.com/2017/01/24/us/politics...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                                lie  \\\n",
       "0  2017-01-21  I wasn't a fan of Iraq. I didn't want to go in...   \n",
       "1  2017-01-21  A reporter for Time magazine — and I have been...   \n",
       "2  2017-01-23  Between 3 million and 5 million illegal votes ...   \n",
       "3  2017-01-25  Now, the audience was the biggest ever. But th...   \n",
       "4  2017-01-25  Take a look at the Pew reports (which show vot...   \n",
       "\n",
       "                                         explanation  \\\n",
       "0   He was for an invasion before he was against it.   \n",
       "1  Trump was on the cover 11 times and Nixon appe...   \n",
       "2             There's no evidence of illegal voting.   \n",
       "3  Official aerial photos show Obama's 2009 inaug...   \n",
       "4            The report never mentioned voter fraud.   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.buzzfeed.com/andrewkaczynski/in-20...  \n",
       "1  http://nation.time.com/2013/11/06/10-things-yo...  \n",
       "2  https://www.nytimes.com/2017/01/23/us/politics...  \n",
       "3  https://www.nytimes.com/2017/01/21/us/politics...  \n",
       "4  https://www.nytimes.com/2017/01/24/us/politics...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "df = pd.read_csv('trump_lies.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we need to know about the headlines that can go viral we will remove other columns from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []  \n",
    "for result in results:  \n",
    "    news = result.contents[1][1:-2]\n",
    "    records.append((news))\n",
    "import pandas as pd  \n",
    "df = pd.DataFrame(records, columns=['news'])   \n",
    "df.to_csv('trump_lies.csv', index=False, encoding='utf-8') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's classify our news whether they will get viral or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I wasn't a fan of Iraq. I didn't want to go into Iraq.\",\n",
       " 'A reporter for Time magazine — and I have been on their cover 14 or 15 times. I think we have the all-time record in the history of Time magazine.',\n",
       " 'Between 3 million and 5 million illegal votes caused me to lose the popular vote.',\n",
       " 'Now, the audience was the biggest ever. But this crowd was massive. Look how far back it goes. This crowd was massive.',\n",
       " 'Take a look at the Pew reports (which show voter fraud.)']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have taken other news headline dataset just to train our model and finally apply that training to our SCRAPED DATASET, So we have used 2 datasets one for training and other for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"viral.txt\",encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "    lines = [line.split(\"\\t\") for line in lines]\n",
    "headlines, labels = zip(*lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Egypt's top envoy in Iraq confirmed killed\",\n",
       " 'Carter: Race relations in Palestine are worse than apartheid',\n",
       " 'After Years Of Dutiful Service, The Shiba Who Ran A Tobacco Shop Retires',\n",
       " 'In Books on Two Powerbrokers, Hints of the Future',\n",
       " 'These Horrifyingly Satisfying Photos Of \"Baby Foot\" Will Haunt You')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', '0', '1', '0', '1')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1= Viral Headline\n",
    "# 0 = Non Viral Headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headlines = headlines[:8000]\n",
    "test_headlines = records[1:180]\n",
    "train_labels = labels[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vectorizer and classifier\n",
    "vectorizer = TfidfVectorizer()\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our text data into numerical vectors\n",
    "train_vectors = vectorizer.fit_transform(train_headlines)\n",
    "test_vectors = vectorizer.transform(test_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier and predict on test set\n",
    "svm.fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A reporter for Time magazine — and I have been on their cover 14 or 15 times. I think we have the all-time record in the history of Time magazine.',\n",
       " 'Between 3 million and 5 million illegal votes caused me to lose the popular vote.',\n",
       " 'Now, the audience was the biggest ever. But this crowd was massive. Look how far back it goes. This crowd was massive.',\n",
       " 'Take a look at the Pew reports (which show voter fraud.)',\n",
       " \"You had millions of people that now aren't insured anymore.\"]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_headlines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final output showing whether the scraped headline will go viral or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '0',\n",
       "       '1', '1', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1',\n",
       "       '0', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0',\n",
       "       '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '0', '1',\n",
       "       '0', '0', '1', '0', '0', '0', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0',\n",
       "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '0', '0', '1', '1', '1', '1', '1', '1'], dtype='<U1')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As you can see there is a lot of '1' which shows that the news will get viral and '0' it won't. The lot of '1' indicate as our test dataset is from New York times and related to America's President Trump, So probably it is showing the correct result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But we can check the accuracy of our trained model using labelled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break viral dataset into test and split\n",
    "train_headlines = headlines[:8000]\n",
    "test_headlines = headlines[8000:]\n",
    "\n",
    "train_labels = labels[:8000]\n",
    "test_labels = labels[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(train_headlines)\n",
    "test_vectors = vectorizer.transform(test_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Earliest I\\'ve Said \"I Love You\"',\n",
       " \"Stop What You're Doing And Worship These Matt Bomer Pictures\",\n",
       " '23 Of The Funniest \"Nancy Drew\" Game Memes',\n",
       " 'Policeman killed in football-related violence in Italy',\n",
       " 'Do You Remember Which Disney Star Sang These Lyrics')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_headlines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '1', '0', '1'], dtype='<U1')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', '1', '1', '0', '1')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.961"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the accuracy of our model is 96% for labelled data and for unlabelled data we have provide label whether it will go viral or not just above. I hope this will somehow justify your problem statement. I thankyou for this opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
